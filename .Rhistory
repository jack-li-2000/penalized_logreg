# find the prob
prob = exp(logit) / (1+exp(logit))
# find the gradients
gradm = (1/n) * (t(-data_label + prob)%*%data_feature) + (lbd*sum(beta))
grad0 = (1/n) * sum(-data_label + prob)
# compile gradients
grad = c(grad0, gradm)
#####################################################################
#                       END OF YOUR CODE                            #
#####################################################################
return(grad)
}
Comp_loss <- function(data_feature, data_label, beta, beta0, lbd) {
# Compute and return the loss of the penalized logistic regression
#
# Note: n is the number of examples
#       p is the number of features per example
#
# @param data_feature: A matrix with dimension n x p, where each row corresponds to
#   one data point.
# @param data_label: A vector of labels with with length equal to n.
# @param beta: A vector of coefficients with length equal to p.
# @param beta0: the intercept.
# @param lbd: the regularization parameter
#
# @return: a value of the loss function
#####################################################################
# TODO:                                                             #
#####################################################################
# TODO: check if formula is valid
loss <- 0
n = nrow(data_feature)
# find the logit
logit = beta0 + (data_feature)%*%beta
# find the prob
prob = exp(logit) / (1+exp(logit))
# find loss
loss = ((-1/n) * sum((data_label * log(prob)) + ((1 - data_label)*log(1-prob)))) + ((lbd/2)*sum(beta^2))
#####################################################################
#                       END OF YOUR CODE                            #
#####################################################################
return(loss)
}
Penalized_Logistic_Reg <- function(x_train, y_train, lbd, stepsize, max_iter) {
# This is the main function to fit the Penalized Logistic Regression
#
# Note: n is the number of examples
#       p is the number of features per example
#
# @param x_train: A matrix with dimension n x p, where each row corresponds to
#   one training point.
# @param y_train: A vector of labels with length equal to n.
# @param lbd: the regularization parameter.
# @param stepsize: the learning rate.
# @param max_iter: a positive integer specifying the maximal number of
#   iterations.
#
# @return: a list containing four components:
#   loss: a vector of loss values at each iteration
#   error: a vector of 0-1 errors at each iteration
#   beta: the estimated p coefficient vectors
#   beta0: the estimated intercept.
p <- ncol(x_train)
# Initialize parameters to 0
beta_cur <- rep(0, p)
beta0_cur <- 0
# Create the vectors for recording values of loss and 0-1 error during
# the training procedure
loss_vec <- rep(0, max_iter)
error_vec <- rep(0, max_iter)
#####################################################################
# TODO:                                                             #
# Modify this section to perform gradient descent and to compute    #
# losses and 0-1 errors at each iterations.                         #
#####################################################################
for (i in 1:max_iter){
grad = Comp_gradient(x_train, y_train, beta_cur, beta0_cur, lbd)
beta0_cur = beta0_cur - stepsize*(grad[1])
beta_cur = beta_cur - stepsize*(grad[-1])
loss_vec[i] = Comp_loss(x_train,y_train,beta_cur,beta0_cur,lbd)
pred = Predict_logis(x_train, beta_cur, beta0_cur, 'label')
error_vec[i] = Evaluate(y_train,pred)
}
#####################################################################
#                       END OF YOUR CODE                            #
#####################################################################
return(list("loss" = loss_vec, "error" = error_vec,
"beta" = beta_cur, "beta0" = beta0_cur))
}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
## You should set the working directory to the folder of hw3_starter by
## uncommenting the following and replacing YourDirectory by what you have
## in your local computer / labtop
setwd("D:/OneDrive - University of Toronto/university/4th year/sta314/hw3/hw3_starter/hw3_starter")
## Load utils.R and penalized_logistic_regression.R
source("utils.R")
source("penalized_logistic_regression.R")
## load data sets
train <- Load_data("./data/train.csv")
valid <- Load_data("./data/valid.csv")
test <- Load_data("./data/test.csv")
x_train <- train$x
y_train <- train$y
x_valid <- valid$x
y_valid <- valid$y
x_test <- test$x
y_test <- test$y
#####################################################################
#                           Part a.                                 #
# TODO: Find the best choice of the hyperparameters:                #
#     - stepsize (i.e. the learning rate)                           #
#     - max_iter (the maximal number of iterations)                 #
#   The regularization parameter, lbd, should be set to 0           #
#   Draw plot of training losses and training 0-1 errors            #
#####################################################################
lbd = 0
stepsize = 1/5
max_iter = 50
result = Penalized_Logistic_Reg(x_train, y_train, lbd, stepsize, max_iter)
plot(result$loss, main='training loss vs iterations', xlab='iterations', ylab='loss')
plot(result$error, main='0-1 error vs iterations', xlab='iterations', ylab='error')
#####################################################################
#                       END OF YOUR CODE                            #
#####################################################################
#####################################################################
#                           Part b.                                 #
# TODO: Identify the best stepsize and max_iter for each lambda     #
#       from the given grid. Draw the plots of training and         #
#       validation 0-1 errors versus different values of lambda     #
#####################################################################
stepsize <- c(1, 3/5, 1/11, 1/15, 1/75, 1/150) # this should be replaced by your answer in Part a
max_iter <- c(100, 100, 100, 100, 300, 400)  # this should be replaced by your answer in Part a
lbd_grid <- c(0, 0.01, 0.05, 0.1, 0.5, 1)
result = Penalized_Logistic_Reg(x_train, y_train, lbd_grid[1], stepsize[1], max_iter[1])
plot(result$error)
resultv = Penalized_Logistic_Reg(x_valid, y_valid, lbd_grid[1], stepsize[1], max_iter[1])
plot(resultv$error)
#####################################################################
#                       END OF YOUR CODE                            #
#####################################################################
result2 = Penalized_Logistic_Reg(x_train, y_train, lbd_grid[2], stepsize[2], max_iter[2])
plot(result2$error)
result2v = Penalized_Logistic_Reg(x_valid, y_valid, lbd_grid[2], stepsize[2], max_iter[2])
plot(result2v$error)
result3 = Penalized_Logistic_Reg(x_train, y_train, lbd_grid[3], stepsize[3], max_iter[3])
plot(result3$error)
result3v = Penalized_Logistic_Reg(x_valid, y_valid, lbd_grid[3], stepsize[3], max_iter[3])
plot(result3v$error)
result4 = Penalized_Logistic_Reg(x_train, y_train, lbd_grid[4], stepsize[4], max_iter[4])
plot(result4$error)
result4v = Penalized_Logistic_Reg(x_valid, y_valid, lbd_grid[4], stepsize[4], max_iter[4])
plot(result4v$error)
result5 = Penalized_Logistic_Reg(x_train, y_train, lbd_grid[5], stepsize[5], max_iter[5])
plot(result5$error)
result5v = Penalized_Logistic_Reg(x_valid, y_valid, lbd_grid[5], stepsize[5], max_iter[5])
plot(result5v$error)
result6 = Penalized_Logistic_Reg(x_train, y_train, lbd_grid[6], stepsize[6], max_iter[6])
plot(result6$error, main='training max_iter - 400, learning - 1/150')
result6v = Penalized_Logistic_Reg(x_valid, y_valid, lbd_grid[6], stepsize[6], max_iter[6])
plot(result6v$error)
result6 = Penalized_Logistic_Reg(x_train, y_train, lbd_grid[6], stepsize[6], max_iter[6])
plot(result6$error, title='training max_iter - 400, learning - 1/150')
result6v = Penalized_Logistic_Reg(x_valid, y_valid, lbd_grid[6], stepsize[6], max_iter[6])
plot(result6v$error)
result6 = Penalized_Logistic_Reg(x_train, y_train, lbd_grid[6], stepsize[6], max_iter[6])
plot(result6$error, main='training max_iter - 400, learning - 1/150')
result6v = Penalized_Logistic_Reg(x_valid, y_valid, lbd_grid[6], stepsize[6], max_iter[6])
plot(result6v$error)
Evaluate <- function(true_label, pred_label) {
#  Compute the 0-1 loss between two vectors
#
#  @param true_label: A vector of true labels with length n
#  @param pred_label: A vector of predicted labels with length n
#  @return: fraction of points get misclassified
#####################################################################
#  TODO                                                             #
#####################################################################
error <- 0
error = sum(true_label!=pred_label)/length(pred_label)
#####################################################################
#                       END OF YOUR CODE                            #
#####################################################################
return(error)
}
Predict_logis <- function(data_feature, beta, beta0, type) {
# Predict by the logistic classifier.
#
# Note: n is the number of examples
#       p is the number of features per example
#
# @param data_feature: A matrix with dimension n x p, where each row corresponds to
#   one data point.
# @param beta: A vector of coefficients with length equal to p.
# @param beta0: the intercept.
# @param type: a string value within {"logit", "prob", "class"}.
# @return: A vector with length equal to n, consisting of
#   predicted logits,         if type = "logit";
#   predicted probabilities,  if type = "prob";
#   predicted labels,         if type = "class".
n <- nrow(data_feature)
pred_vec <- rep(0, n)
#####################################################################
#  TODO                                                             #
#####################################################################
# TODO: check if data_feature and *betas match orientation and check if
#       colSums works as intended
# finding values
logit = beta0 + colSums(t(data_feature)*beta)
prob = exp(logit) / (1+exp(logit))
label = rep(0,n)
label[prob>=0.5] = 1
# reporting values
if (type == 'logit'){pred_vec = logit}
else if (type == 'prob'){pred_vec = prob}
else {pred_vec = label}
#####################################################################
#                       END OF YOUR CODE                            #
#####################################################################
return(pred_vec)
}
Comp_gradient <- function(data_feature, data_label, beta, beta0, lbd) {
# Compute and return the gradient of the penalized logistic regression
#
# Note: n is the number of examples
#       p is the number of features per example
#
# @param data_feature: A matrix with dimension n x p, where each row corresponds to
#   one data point.
# @param data_label: A vector of labels with length equal to n.
# @param beta: A vector of coefficients with length equal to p.
# @param beta0: the intercept.
# @param lbd: the regularization parameter
#
# @return: a (p+1) x 1 vector of gradients, the first coordinate is the gradient
#   w.r.t. the intercept.
n <- nrow(data_feature)
p <- ncol(data_feature)
grad <- rep(0, 1 + p)
#####################################################################
# TODO:                                                             #
#####################################################################
# find the logit
logit = beta0 + (data_feature)%*%beta
# find the prob
prob = exp(logit) / (1+exp(logit))
# find the gradients
gradm = (1/n) * (t(-data_label + prob)%*%data_feature) + (lbd*sum(beta))
grad0 = (1/n) * sum(-data_label + prob)
# compile gradients
grad = c(grad0, gradm)
#####################################################################
#                       END OF YOUR CODE                            #
#####################################################################
return(grad)
}
Comp_loss <- function(data_feature, data_label, beta, beta0, lbd) {
# Compute and return the loss of the penalized logistic regression
#
# Note: n is the number of examples
#       p is the number of features per example
#
# @param data_feature: A matrix with dimension n x p, where each row corresponds to
#   one data point.
# @param data_label: A vector of labels with with length equal to n.
# @param beta: A vector of coefficients with length equal to p.
# @param beta0: the intercept.
# @param lbd: the regularization parameter
#
# @return: a value of the loss function
#####################################################################
# TODO:                                                             #
#####################################################################
# TODO: check if formula is valid
loss <- 0
n = nrow(data_feature)
# find the logit
logit = beta0 + (data_feature)%*%beta
# find the prob
prob = exp(logit) / (1+exp(logit))
# find loss
loss = ((-1/n) * sum((data_label * log(prob)) + ((1 - data_label)*log(1-prob)))) + ((lbd/2)*sum(beta^2))
#####################################################################
#                       END OF YOUR CODE                            #
#####################################################################
return(loss)
}
Penalized_Logistic_Reg <- function(x_train, y_train, lbd, stepsize, max_iter) {
# This is the main function to fit the Penalized Logistic Regression
#
# Note: n is the number of examples
#       p is the number of features per example
#
# @param x_train: A matrix with dimension n x p, where each row corresponds to
#   one training point.
# @param y_train: A vector of labels with length equal to n.
# @param lbd: the regularization parameter.
# @param stepsize: the learning rate.
# @param max_iter: a positive integer specifying the maximal number of
#   iterations.
#
# @return: a list containing four components:
#   loss: a vector of loss values at each iteration
#   error: a vector of 0-1 errors at each iteration
#   beta: the estimated p coefficient vectors
#   beta0: the estimated intercept.
p <- ncol(x_train)
# Initialize parameters to 0
beta_cur <- rep(0, p)
beta0_cur <- 0
# Create the vectors for recording values of loss and 0-1 error during
# the training procedure
loss_vec <- rep(0, max_iter)
error_vec <- rep(0, max_iter)
#####################################################################
# TODO:                                                             #
# Modify this section to perform gradient descent and to compute    #
# losses and 0-1 errors at each iterations.                         #
#####################################################################
for (i in 1:max_iter){
grad = Comp_gradient(x_train, y_train, beta_cur, beta0_cur, lbd)
beta0_cur = beta0_cur - stepsize*(grad[1])
beta_cur = beta_cur - stepsize*(grad[-1])
loss_vec[i] = Comp_loss(x_train,y_train,beta_cur,beta0_cur,lbd)
pred = Predict_logis(x_train, beta_cur, beta0_cur, 'label')
error_vec[i] = Evaluate(y_train,pred)
}
#####################################################################
#                       END OF YOUR CODE                            #
#####################################################################
return(list("loss" = loss_vec, "error" = error_vec,
"beta" = beta_cur, "beta0" = beta0_cur))
}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
## You should set the working directory to the folder of hw3_starter by
## uncommenting the following and replacing YourDirectory by what you have
## in your local computer / labtop
setwd("D:/OneDrive - University of Toronto/university/4th year/sta314/hw3/hw3_starter/hw3_starter")
## Load utils.R and penalized_logistic_regression.R
source("utils.R")
source("penalized_logistic_regression.R")
## load data sets
train <- Load_data("./data/train.csv")
valid <- Load_data("./data/valid.csv")
test <- Load_data("./data/test.csv")
x_train <- train$x
y_train <- train$y
x_valid <- valid$x
y_valid <- valid$y
x_test <- test$x
y_test <- test$y
#####################################################################
#                           Part a.                                 #
# TODO: Find the best choice of the hyperparameters:                #
#     - stepsize (i.e. the learning rate)                           #
#     - max_iter (the maximal number of iterations)                 #
#   The regularization parameter, lbd, should be set to 0           #
#   Draw plot of training losses and training 0-1 errors            #
#####################################################################
lbd = 0
stepsize = 1/5
max_iter = 50
result = Penalized_Logistic_Reg(x_train, y_train, lbd, stepsize, max_iter)
plot(result$loss, main='training loss vs iterations', xlab='iterations', ylab='loss')
plot(result$error, main='0-1 error vs iterations', xlab='iterations', ylab='error')
#####################################################################
#                       END OF YOUR CODE                            #
#####################################################################
#####################################################################
#                           Part b.                                 #
# TODO: Identify the best stepsize and max_iter for each lambda     #
#       from the given grid. Draw the plots of training and         #
#       validation 0-1 errors versus different values of lambda     #
#####################################################################
stepsize <- c(1, 3/5, 1/11, 1/15, 1/75, 1/150) # this should be replaced by your answer in Part a
max_iter <- c(100, 100, 100, 100, 300, 400)  # this should be replaced by your answer in Part a
lbd_grid <- c(0, 0.01, 0.05, 0.1, 0.5, 1)
result = Penalized_Logistic_Reg(x_train, y_train, lbd_grid[1], stepsize[1], max_iter[1])
plot(result$error, main='training max_iter - 100, learning - 1')
resultv = Penalized_Logistic_Reg(x_valid, y_valid, lbd_grid[1], stepsize[1], max_iter[1])
plot(resultv$error, main='valid max_iter - 100, learning - 1')
#####################################################################
#                       END OF YOUR CODE                            #
#####################################################################
result2 = Penalized_Logistic_Reg(x_train, y_train, lbd_grid[2], stepsize[2], max_iter[2])
plot(result2$error, main='training max_iter - 100, learning - 3/5')
result2v = Penalized_Logistic_Reg(x_valid, y_valid, lbd_grid[2], stepsize[2], max_iter[2])
plot(result2v$error, main='valid max_iter - 100, learning - 3/5')
result3 = Penalized_Logistic_Reg(x_train, y_train, lbd_grid[3], stepsize[3], max_iter[3])
plot(result3$error, main='training max_iter - 100, learning - 1/11')
result3v = Penalized_Logistic_Reg(x_valid, y_valid, lbd_grid[3], stepsize[3], max_iter[3])
plot(result3v$error, main='valid max_iter - 100, learning - 1/11')
result4 = Penalized_Logistic_Reg(x_train, y_train, lbd_grid[4], stepsize[4], max_iter[4])
plot(result4$error, main='training max_iter - 100, learning - 1/15')
result4v = Penalized_Logistic_Reg(x_valid, y_valid, lbd_grid[4], stepsize[4], max_iter[4])
plot(result4v$error, main='valid max_iter - 100, learning - 1/15')
result5 = Penalized_Logistic_Reg(x_train, y_train, lbd_grid[5], stepsize[5], max_iter[5])
plot(result5$error, main='training max_iter - 300, learning - 1/75')
result5v = Penalized_Logistic_Reg(x_valid, y_valid, lbd_grid[5], stepsize[5], max_iter[5])
plot(result5v$error, main='valid max_iter - 300, learning - 1/75')
result6 = Penalized_Logistic_Reg(x_train, y_train, lbd_grid[6], stepsize[6], max_iter[6])
plot(result6$error, main='training max_iter - 400, learning - 1/150')
result6v = Penalized_Logistic_Reg(x_valid, y_valid, lbd_grid[6], stepsize[6], max_iter[6])
plot(result6v$error, main='valid max_iter - 400, learning - 1/150')
#####################################################################
#                           Part c.                                 #
# TODO: using the best stepsize,  max_iter and lbd you found, fit   #
#  the penalized logistic regression and compute its test 0-1 error #
#####################################################################
stepsize <- 1  # this should be replaced by your answer in Part a
max_iter <- 100  # this should be replaced by your answer in Part a
lbd <- 0      # this should be replaced by your answer in Part b
result_test = Penalized_Logistic_Reg(x_test, y_test, lbd, stepsize, max_iter)
plot(result_test$error)
# install.packages('glmnet')
library(glmnet)
glm_test = glmnet(x_train, y_train, family = 'binomial', lambda = 0)
predict(glm_test, x_test, type = 'response')
#####################################################################
#                       END OF YOUR CODE                            #
#####################################################################
setwd("D:\github_repo\penalized_logistic_regression\penalized_logistic_regression\penalized_logreg")
setwd("D:github_repo\penalized_logistic_regression\penalized_logistic_regression\penalized_logreg")
setwd("D:/github_repo/penalized_logistic_regression/penalized_logistic_regression/penalized_logreg")
source("utils.R")
rm(list = ls())
setwd("D:/github_repo/penalized_logistic_regression/penalized_logistic_regression/penalized_logreg")
## Load utils.R and penalized_logistic_regression.R
source("utils.R")
source("penalized_logistic_regression.R")
## load data sets
train <- Load_data("./data/train.csv")
valid <- Load_data("./data/valid.csv")
test <- Load_data("./data/test.csv")
x_train <- train$x
y_train <- train$y
x_valid <- valid$x
y_valid <- valid$y
x_test <- test$x
y_test <- test$y
## visualize the first five and 301th-305th digits in the training data.
Plot_digits(c(1:5, 301:305), x_train)
# checking hyperparameters
lbd = 0
stepsize = 1/5
max_iter = 400
result = Penalized_Logistic_Reg(x_train, y_train, lbd, stepsize, max_iter)
result$loss
result$error
plot(result$error)
# =============================================================================
# =============================================================================
stepsize <- 1
max_iter <- 400
lbd_grid <- c(0, 0.01, 0.05, 0.1, 0.5, 1)
result = Penalized_Logistic_Reg(x_train, y_train, lbd_grid[1], stepsize, max_iter)
plot(result$error)
result2 = Penalized_Logistic_Reg(x_train, y_train, lbd_grid[2], stepsize, max_iter)
plot(result2$error)
result3 = Penalized_Logistic_Reg(x_train, y_train, lbd_grid[3], stepsize, max_iter)
plot(result3$error)
result4 = Penalized_Logistic_Reg(x_train, y_train, lbd_grid[4], stepsize, max_iter)
plot(result4$error)
result5 = Penalized_Logistic_Reg(x_train, y_train, lbd_grid[5], stepsize, max_iter)
plot(result5$error)
result6 = Penalized_Logistic_Reg(x_train, y_train, lbd_grid[6], stepsize, max_iter)
plot(result6$error)
result5$error
# =============================================================================
# =============================================================================
# fit based on best hyperparameters
stepsize <- 0
max_iter <- 0
lbd <- 0
# can also use cross validation to fit hyperparameters
result = Penalized_Logistic_Reg(x_train, y_train, lbd_grid[1], stepsize, max_iter)
plot(result$error)
result2 = Penalized_Logistic_Reg(x_train, y_train, lbd_grid[2], stepsize, max_iter)
plot(result2$error)
result3 = Penalized_Logistic_Reg(x_train, y_train, lbd_grid[3], stepsize, max_iter)
plot(result3$error)
lbd = 0
stepsize = 1/5
max_iter = 400
result = Penalized_Logistic_Reg(x_train, y_train, lbd, stepsize, max_iter)
result$loss
result$error
plot(result$error)
plot(result$loss)
stepsize <- 1/5
max_iter <- 400
lbd_grid <- c(0, 0.01, 0.05, 0.1, 0.5, 1)
result = Penalized_Logistic_Reg(x_train, y_train, lbd_grid[1], stepsize, max_iter)
plot(result$error)
result2 = Penalized_Logistic_Reg(x_train, y_train, lbd_grid[2], stepsize, max_iter)
plot(result2$error)
result3 = Penalized_Logistic_Reg(x_train, y_train, lbd_grid[3], stepsize, max_iter)
plot(result3$error)
result4 = Penalized_Logistic_Reg(x_train, y_train, lbd_grid[4], stepsize, max_iter)
plot(result4$error)
result5 = Penalized_Logistic_Reg(x_train, y_train, lbd_grid[5], stepsize, max_iter)
plot(result5$error)
result6 = Penalized_Logistic_Reg(x_train, y_train, lbd_grid[6], stepsize, max_iter)
plot(result6$error)
result5$error
result3 = Penalized_Logistic_Reg(x_train, y_train, lbd_grid[3], stepsize, max_iter)
plot(result3$error)
result2 = Penalized_Logistic_Reg(x_train, y_train, lbd_grid[2], stepsize, max_iter)
plot(result2$error)
plot(result2$loss)
plot(result2$error)
